"""
Backtest picks generated by historical_picks_runner.

Goal:
- Join picks -> history (to get outcomes)
- Optionally join picks -> market snapshots (to get odds)
- Compute pick-conditioned performance (ROI, PnL, hit rate)
- Audit coverage (history join %, snapshot/odds coverage %, rows dropped)

Commit discipline:
- Do NOT change pick generation logic
- Do NOT change odds ingestion rules upstream
- Snapshot usage is OPTIONAL and auditable
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd


# ---------------------------------------------------------------------
# Core constants / utilities
# ---------------------------------------------------------------------

def _norm_team(x: object) -> str:
    return str(x).strip().lower() if x is not None else ""


def _merge_key(home: object, away: object, game_date: object) -> str:
    return f"{_norm_team(home)}__{_norm_team(away)}__{str(game_date).strip()}"


def _find_score_cols(df: pd.DataFrame) -> Tuple[str, str]:
    # Mirrors the robustness pattern used elsewhere
    candidates = [
        ("home_score", "away_score"),
        ("home_points", "away_points"),
        ("pts_home", "pts_away"),
        ("home_team_score", "away_team_score"),
        ("home_score_final", "away_score_final"),
        ("home_pts", "away_pts"),
    ]
    for h, a in candidates:
        if h in df.columns and a in df.columns:
            return h, a
    raise RuntimeError(
        "[picks_backtest] Could not find home/away score columns in joined data. "
        f"Available columns: {list(df.columns)}"
    )


def _american_to_decimal(odds: float) -> float:
    if odds is None or (isinstance(odds, float) and np.isnan(odds)):
        return np.nan
    try:
        o = float(odds)
    except Exception:
        return np.nan
    if o > 0:
        return 1.0 + (o / 100.0)
    if o < 0:
        return 1.0 + (100.0 / abs(o))
    return np.nan


def _find_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    for c in candidates:
        if c in df.columns:
            return c
    return None


def _load_snapshot_for_date(snapshot_dir: Path, game_date: str) -> Optional[pd.DataFrame]:
    """
    Snapshot naming in your repo: close_YYYYMMDD.csv (and sometimes YYYYMMDD.csv).
    """
    if snapshot_dir is None or not snapshot_dir.exists():
        return None

    ymd = str(game_date).replace("-", "")
    for p in [snapshot_dir / f"close_{ymd}.csv", snapshot_dir / f"{ymd}.csv"]:
        if p.exists():
            df = pd.read_csv(p)
            df["_snapshot_file"] = p.name
            return df
    return None


# ---------------------------------------------------------------------
# Picks normalization + odds attachment
# ---------------------------------------------------------------------

def _normalize_picks(picks_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """
    Convert current/legacy picks outputs to canonical fields needed for backtest:

    Required outputs after normalize:
      - game_date, home_team, away_team
      - merge_key
      - bet_side ("HOME"/"AWAY")
      - pick_type (optional, but kept)
      - odds_decimal (may be NaN until we attach from snapshots)
    """
    df = picks_df.copy()
    audit: Dict = {"inferred": {}, "rows_in": int(len(df))}

    # game_date
    if "game_date" not in df.columns and "date" in df.columns:
        df = df.rename(columns={"date": "game_date"})
        audit["inferred"]["game_date"] = "renamed from date"

    needed_for_key = {"game_date", "home_team", "away_team"}
    missing_key = needed_for_key - set(df.columns)
    if missing_key:
        raise RuntimeError(f"[picks_backtest] Picks missing required columns for join: {sorted(missing_key)}")

    # merge_key
    if "merge_key" not in df.columns:
        df["merge_key"] = [
            _merge_key(h, a, d) for h, a, d in zip(df["home_team"], df["away_team"], df["game_date"])
        ]
        audit["inferred"]["merge_key"] = "built from home_team/away_team/game_date"

    # bet_side
    if "bet_side" not in df.columns:
        # Prefer existing pick_side / side style fields
        if "pick_side" in df.columns:
            df["bet_side"] = df["pick_side"].astype(str).str.upper()
            audit["inferred"]["bet_side"] = "from pick_side"
        elif "side" in df.columns:
            df["bet_side"] = df["side"].astype(str).str.upper()
            audit["inferred"]["bet_side"] = "from side"
        else:
            raise RuntimeError("[picks_backtest] Cannot infer bet_side (need bet_side/pick_side/side)")

    df["bet_side"] = df["bet_side"].astype(str).str.upper()
    df = df[df["bet_side"].isin(["HOME", "AWAY"])].copy()

    # odds: we will attach from snapshots. Keep placeholder column.
    if "odds_decimal" not in df.columns:
        df["odds_decimal"] = np.nan

    audit["rows_out"] = int(len(df))
    return df, audit


def _attach_odds_from_snapshots(
    picks_df: pd.DataFrame,
    snapshot_dir: Optional[Path],
) -> Tuple[pd.DataFrame, Dict]:
    """
    Option A:
    Fill odds_decimal using snapshot moneylines.

    We do NOT change odds rules. We only read snapshot CSVs and map:
      HOME pick -> home moneyline (close preferred, fallback open)
      AWAY pick -> away moneyline (close preferred, fallback open)
    """
    df = picks_df.copy()
    audit: Dict = {
        "snapshot_dir": str(snapshot_dir) if snapshot_dir else None,
        "dates_seen": int(df["game_date"].nunique()),
        "snapshots_found": 0,
        "odds_filled": 0,
        "odds_missing_after": 0,
        "columns_used": {},
    }

    if snapshot_dir is None:
        audit["note"] = "snapshot_dir not provided; odds not attached"
        audit["odds_missing_after"] = int(df["odds_decimal"].isna().sum())
        return df, audit

    # Candidate columns (flexible)
    home_ml_close_candidates = ["home_moneyline_close", "close_home_ml", "home_ml_close", "home_moneyline"]
    away_ml_close_candidates = ["away_moneyline_close", "close_away_ml", "away_ml_close", "away_moneyline"]
    home_ml_open_candidates = ["home_moneyline_open", "open_home_ml", "home_ml_open", "home_moneyline_opening"]
    away_ml_open_candidates = ["away_moneyline_open", "open_away_ml", "away_ml_open", "away_moneyline_opening"]

    filled = 0
    found_snaps = 0

    for game_date, idx in df.groupby("game_date").groups.items():
        snap = _load_snapshot_for_date(snapshot_dir, str(game_date))
        if snap is None or snap.empty:
            continue

        found_snaps += 1

        # team columns in snapshot
        home_team_col = _find_col(snap, ["home_team", "home", "team_home"])
        away_team_col = _find_col(snap, ["away_team", "away", "team_away"])
        if not home_team_col or not away_team_col:
            continue

        snap["_mk"] = [
            _merge_key(h, a, game_date) for h, a in zip(snap[home_team_col], snap[away_team_col])
        ]
        snap = snap.set_index("_mk")

        home_close_col = _find_col(snap, home_ml_close_candidates)
        away_close_col = _find_col(snap, away_ml_close_candidates)
        home_open_col = _find_col(snap, home_ml_open_candidates)
        away_open_col = _find_col(snap, away_ml_open_candidates)

        # record columns used once
        if not audit["columns_used"]:
            audit["columns_used"] = {
                "home_team_col": home_team_col,
                "away_team_col": away_team_col,
                "home_ml_close_col": home_close_col,
                "away_ml_close_col": away_close_col,
                "home_ml_open_col": home_open_col,
                "away_ml_open_col": away_open_col,
            }

        for i in idx:
            mk = df.at[i, "merge_key"]
            if mk not in snap.index:
                continue

            row = snap.loc[mk]

            # Prefer close, fallback to open
            if df.at[i, "bet_side"] == "HOME":
                am = row.get(home_close_col, np.nan) if home_close_col else np.nan
                if pd.isna(am) and home_open_col:
                    am = row.get(home_open_col, np.nan)
            else:
                am = row.get(away_close_col, np.nan) if away_close_col else np.nan
                if pd.isna(am) and away_open_col:
                    am = row.get(away_open_col, np.nan)

            dec = _american_to_decimal(am)
            if pd.notna(dec):
                df.at[i, "odds_decimal"] = float(dec)
                filled += 1

    audit["snapshots_found"] = int(found_snaps)
    audit["odds_filled"] = int(filled)
    audit["odds_missing_after"] = int(df["odds_decimal"].isna().sum())
    return df, audit


# ---------------------------------------------------------------------
# Backtest
# ---------------------------------------------------------------------

def backtest_picks(
    picks_df: pd.DataFrame,
    history_df: pd.DataFrame,
    *,
    snapshot_dir: Optional[Path],
    stake: float,
) -> Tuple[pd.DataFrame, Dict]:
    # Normalize picks schema
    picks_norm, norm_audit = _normalize_picks(picks_df)

    # Attach odds from snapshots (Option A)
    picks_w_odds, odds_audit = _attach_odds_from_snapshots(picks_norm, snapshot_dir)

    # Join to history (must contain merge_key too, or we'll build it)
    hist = history_df.copy()
    if "merge_key" not in hist.columns:
        required_hist = {"game_date", "home_team", "away_team"}
        if "game_date" not in hist.columns and "date" in hist.columns:
            hist = hist.rename(columns={"date": "game_date"})
        miss = required_hist - set(hist.columns)
        if miss:
            raise RuntimeError(f"[picks_backtest] History missing columns needed for merge_key: {sorted(miss)}")
        hist["merge_key"] = [
            _merge_key(h, a, d) for h, a, d in zip(hist["home_team"], hist["away_team"], hist["game_date"])
        ]

    joined = picks_w_odds.merge(
        hist,
        on="merge_key",
        how="left",
        validate="many_to_one",
        indicator=True,
    )

    join_audit = {
        "picks_rows": int(len(picks_w_odds)),
        "joined_rows": int(len(joined)),
        "missing_history": int((joined["_merge"] != "both").sum()),
    }

    joined = joined[joined["_merge"] == "both"].drop(columns="_merge")
    if joined.empty:
        raise RuntimeError("[picks_backtest] All picks dropped after history join")

    # Outcome
    home_score_col, away_score_col = _find_score_cols(joined)
    joined["home_win"] = (joined[home_score_col] > joined[away_score_col]).astype(int)

    joined["result"] = joined.apply(
        lambda r: r["home_win"] if r["bet_side"] == "HOME" else 1 - r["home_win"],
        axis=1,
    )

    # PnL only when odds are present
    joined["stake"] = float(stake)
    joined["pnl"] = np.where(
        joined["odds_decimal"].notna(),
        np.where(joined["result"] == 1, (joined["odds_decimal"] - 1.0) * joined["stake"], -joined["stake"]),
        np.nan,
    )

    resolved = joined[joined["pnl"].notna()].copy()

    audit = {
        "normalize": norm_audit,
        "odds_attach": odds_audit,
        "history_join": join_audit,
        "resolved_bets": int(len(resolved)),
        "unresolved_missing_odds": int(joined["pnl"].isna().sum()),
    }

    if resolved.empty:
        raise RuntimeError(
            "[picks_backtest] No resolved bets (likely missing odds from snapshots). "
            "Check snapshot coverage and moneyline column names."
        )

    # Summary stats
    total_pnl = float(resolved["pnl"].sum())
    total_staked = float(resolved["stake"].sum())
    roi = (total_pnl / total_staked) if total_staked > 0 else float("nan")

    audit["summary"] = {
        "bets": int(len(resolved)),
        "wins": int((resolved["result"] == 1).sum()),
        "losses": int((resolved["result"] == 0).sum()),
        "total_pnl": total_pnl,
        "total_staked": total_staked,
        "roi": roi,
    }

    return resolved, audit


# ---------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------

def main() -> int:
    ap = argparse.ArgumentParser(description="Backtest picks (with optional snapshot odds attachment).")
    ap.add_argument("--picks-dir", required=True)
    ap.add_argument("--pattern", default="picks_*.csv")
    ap.add_argument("--history", required=True)
    ap.add_argument("--snapshot-dir", default=None)  # MUST remain accepted
    ap.add_argument("--out-dir", required=True)
    ap.add_argument("--stake", type=float, default=1.0)
    args = ap.parse_args()

    picks_dir = Path(args.picks_dir)
    files = sorted(picks_dir.glob(args.pattern))
    if not files:
        raise RuntimeError(f"[picks_backtest] No picks files matched: {picks_dir}/{args.pattern}")

    dfs = []
    for f in files:
        df = pd.read_csv(f)
        if df is not None and not df.empty:
            df["_source_file"] = f.name
            dfs.append(df)

    if not dfs:
        raise RuntimeError("[picks_backtest] All picks files were empty")

    picks = pd.concat(dfs, ignore_index=True)
    history = pd.read_csv(args.history)

    snap_dir = Path(args.snapshot_dir) if args.snapshot_dir else None

    joined, audit = backtest_picks(
        picks,
        history,
        snapshot_dir=snap_dir,
        stake=float(args.stake),
    )

    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    joined_path = out_dir / "picks_backtest.csv"
    audit_path = out_dir / "picks_backtest_audit.json"
    summary_path = out_dir / "picks_backtest_summary.csv"

    joined.to_csv(joined_path, index=False)
    audit_path.write_text(json.dumps(audit, indent=2), encoding="utf-8")

    summary = pd.DataFrame([audit["summary"]])
    summary.to_csv(summary_path, index=False)

    print(f"[picks_backtest] wrote {joined_path}")
    print(f"[picks_backtest] wrote {summary_path}")
    print(f"[picks_backtest] wrote {audit_path}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
